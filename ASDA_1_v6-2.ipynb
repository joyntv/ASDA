{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3061286c-d2f9-496b-b107-f0e4e52ee694",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The advantage of using our multivariate statistical clustering procedure over the forms of clustering is that classifies papers into.\n",
      "\n",
      "Initiated by the groundbreaking ideas of Boulding (1966), Daly(1968), Georgescu-Roegen (1971) and the publications of The Limits to Growth(Meadows et.al., 1972), today’s scientific, societal and political, discourse, around how a sustainable economy can be achieved is domi-nated by two major strands. One can be subsumed by the umbrella term of degrowth, and the other one centers around the idea of green growth (Jackson, 2009). The major fault line between these strands is their stance towards necessity and desirability of some form of economic growth. \n",
      "Proponents of green growth generally argue that economic activity can grow in spite of physical planetary boundaries. The main argument of this approach is the theoretical possibility of an absolute decoupling of economic growth and environmental impacts in certain assumptions are met(Jackson, 2009; Kallis et al., 2018). Here, absolute decoupling means economic growth in consumption with decreasing, resource consumption in absolute terms(e.g., Jackson, 2009).\n",
      "\n",
      "On the other hand not only does the proponents degrowth critize \n",
      "\n",
      "green growths focus on the growth, they generally argue that an existing economic system that is mainly focused on the growth is the main reason for the increasing exceedance of planetary boundaries (Hickel and Kallies, 2020). Therefore, the degrowth literature organises around two dimensions around two direct criticisms of central tenets of the growth. First, based on  Jevon’s paradox, degrowth, scholars, doubt that efficiency could 1 We are aware that paradox, degrowth research is also published in other ways, in particular books. Yet, it also holds that peer-reviewed journal articles are the most important (and prestigious) outlet in many research fields, including economics.\n",
      "\n",
      "increase a decoupling of resource use and production output(e.g., Dominger et al., 2021). Secondly, based on Georgescu Roegen (1975), they maintain  that  the much needed switch from fossils of renew-ables would come with the decrease in energy available which would entail the decrease in economic production. \n",
      "the concept of stationary economy can be as a complement to the degrowth rather than a synonym(Martinez Alier., 2010). Some have argued that the aim of degrowth is socially and ecologically just transformation of economic activity which itself remains within socially and ecologically viable boundaries (Schneder et al., 2010). In this understanding, degro\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "Double space found\n",
      "['the', 'advantage', 'of', 'using', 'our', 'multivariate', 'statistical', 'clustering', 'procedure', 'over', 'the', 'forms', 'of', 'clustering', 'is', 'that', 'classifies', 'papers', 'intoinitiated', 'by', 'the', 'groundbreaking', 'ideas', 'of', 'boulding', 'daly', 'georgescu-roegen', 'and', 'the', 'publications', 'of', 'the', 'limits', 'to', 'growthmeadows', 'etal', 'today’s', 'scientific', 'societal', 'and', 'political', 'discourse', 'around', 'how', 'a', 'sustainable', 'economy', 'can', 'be', 'achieved', 'is', 'domi-nated', 'by', 'two', 'major', 'strands', 'one', 'can', 'be', 'subsumed', 'by', 'the', 'umbrella', 'term', 'of', 'degrowth', 'and', 'the', 'other', 'one', 'centers', 'around', 'the', 'idea', 'of', 'green', 'growth', 'jackson', 'the', 'major', 'fault', 'line', 'between', 'these', 'strands', 'is', 'their', 'stance', 'towards', 'necessity', 'and', 'desirability', 'of', 'some', 'form', 'of', 'economic', 'growth', 'proponents', 'of', 'green', 'growth', 'generally', 'argue', 'that', 'economic', 'activity', 'can', 'grow', 'in', 'spite', 'of', 'physical', 'planetary', 'boundaries', 'the', 'main', 'argument', 'of', 'this', 'approach', 'is', 'the', 'theoretical', 'possibility', 'of', 'an', 'absolute', 'decoupling', 'of', 'economic', 'growth', 'and', 'environmental', 'impacts', 'in', 'certain', 'assumptions', 'are', 'metjackson', 'kallis', 'et', 'al', 'here', 'absolute', 'decoupling', 'means', 'economic', 'growth', 'in', 'consumption', 'with', 'decreasing', 'resource', 'consumption', 'in', 'absolute', 'termseg', 'jackson', 'on', 'the', 'other', 'hand', 'not', 'only', 'does', 'the', 'proponents', 'degrowth', 'critize', 'green', 'growths', 'focus', 'on', 'the', 'growth', 'they', 'generally', 'argue', 'that', 'an', 'existing', 'economic', 'system', 'that', 'is', 'mainly', 'focused', 'on', 'the', 'growth', 'is', 'the', 'main', 'reason', 'for', 'the', 'increasing', 'exceedance', 'of', 'planetary', 'boundaries', 'hickel', 'and', 'kallies', 'therefore', 'the', 'degrowth', 'literature', 'organises', 'around', 'two', 'dimensions', 'around', 'two', 'direct', 'criticisms', 'of', 'central', 'tenets', 'of', 'the', 'growth', 'first', 'based', 'on', 'jevon’s', 'paradox', 'degrowth', 'scholars', 'doubt', 'that', 'efficiency', 'could', 'we', 'are', 'aware', 'that', 'paradox', 'degrowth', 'research', 'is', 'also', 'published', 'in', 'other', 'ways', 'in', 'particular', 'books', 'yet', 'it', 'also', 'holds', 'that', 'peer-reviewed', 'journal', 'articles', 'are', 'the', 'most', 'important', 'and', 'prestigious', 'outlet', 'in', 'many', 'research', 'fields', 'including', 'economicsincrease', 'a', 'decoupling', 'of', 'resource', 'use', 'and', 'production', 'outputeg', 'dominger', 'et', 'al', 'secondly', 'based', 'on', 'georgescu', 'roegen', 'they', 'maintain', 'that', 'the', 'much', 'needed', 'switch', 'from', 'fossils', 'of', 'renew-ables', 'would', 'come', 'with', 'the', 'decrease', 'in', 'energy', 'available', 'which', 'would', 'entail', 'the', 'decrease', 'in', 'economic', 'production', 'the', 'concept', 'of', 'stationary', 'economy', 'can', 'be', 'as', 'a', 'complement', 'to', 'the', 'degrowth', 'rather', 'than', 'a', 'synonymmartinez', 'alier', 'some', 'have', 'argued', 'that', 'the', 'aim', 'of', 'degrowth', 'is', 'socially', 'and', 'ecologically', 'just', 'transformation', 'of', 'economic', 'activity', 'which', 'itself', 'remains', 'within', 'socially', 'and', 'ecologically', 'viable', 'boundaries', 'schneder', 'et', 'al', 'in', 'this', 'understanding', 'degro']\n"
     ]
    }
   ],
   "source": [
    "# pip install json5\n",
    "\n",
    "########################################\n",
    "# Loading in the import Libraries\n",
    "########################################\n",
    "\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import pandas as pd \n",
    "\n",
    "########################################\n",
    "# Opening and Reading the textfile\n",
    "########################################\n",
    "file1 = open(\"data-science_master/ASDA Data.txt\",\"r\")\n",
    "string_output=str(file1.read())\n",
    "file_path = \"data-science_master/ASDA Data.txt\"\n",
    "\n",
    "# Tests to make sure that the input file is read in correctly\n",
    "print(string_output)\n",
    "\n",
    "########################################\n",
    "#  Cleaning the String to base components\n",
    "########################################\n",
    "List=[]\n",
    "\n",
    "#putting everything in lowercase\n",
    "string_output=string_output.strip(\"  \")\n",
    "# Iterates through letters and checks if the characters in the string are \"'\", a letter, a dash or a space\n",
    "for letter in string_output[0:len(string_output)]:\n",
    "    if ((letter==\"'\") or (letter==\"’\") or (letter==\"‘\") or (letter.isalpha()==True) or (letter==\"-\") or (letter==\" \")):\n",
    "        if letter==\"'\":\n",
    "            print(\"found\")\n",
    "        if letter==\" \" and List[len(List)-1]==\" \":\n",
    "            print(\"Double space found\")\n",
    "        else:\n",
    "            List.append(letter)\n",
    "\n",
    "build_string=\"\".join(List)\n",
    "# converts everything into lowercase, we can decide how to implement\n",
    "build_string = build_string.lower()\n",
    "new_list=build_string.split(\" \")\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b6e152c-ff4d-45bf-ae60-a5e88f6ae494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today’s', 'jevon’s']\n",
      "['today’s', 'jevon’s']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_word_occurrences(new_list):\n",
    "\n",
    "    contractions = {}\n",
    "    contraction_pattern = re.compile(r\"\\b\\w*’\\w*\\b\")\n",
    "\n",
    "    if new_list is not None and isinstance(new_list, list):\n",
    "        new_list = \" \".join(new_list)  # Join list into a single string\n",
    "        contractions = contraction_pattern.findall(new_list)\n",
    "        print(contractions)\n",
    "    elif isinstance(new_list, str):\n",
    "        contractions = contraction_pattern.findall(new_list)\n",
    "        print(contractions)\n",
    "    else:\n",
    "        print(\"new_list is not a valid string or list\")\n",
    "    return contractions\n",
    "\n",
    "count_word_occurrences(new_list)\n",
    "contractions_list = count_word_occurrences(new_list)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78f9132d-1df1-413f-a099-691adb039eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['georgescu-roegen', 'domi-nated', 'peer-reviewed', 'renew-ables']\n",
      "['georgescu-roegen', 'domi-nated', 'peer-reviewed', 'renew-ables']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dashes\n",
    "def count_word_occurrences_2(new_list):\n",
    "\n",
    "    dashes = {}\n",
    "    dashes_pattern = re.compile(r\"\\b\\w*-\\w*\\b\")\n",
    "\n",
    "    if new_list is not None and isinstance(new_list, list):\n",
    "        new_list = \" \".join(new_list)  # Join list into a single string\n",
    "        dashes = dashes_pattern.findall(new_list)\n",
    "        print(dashes)\n",
    "    elif isinstance(new_list, str):\n",
    "        dashes = dashes_pattern.findall(new_list)\n",
    "        print(dashes)\n",
    "    else:\n",
    "        print(\"new_list is not a valid string or list\")\n",
    "    return dashes\n",
    "\n",
    "count_word_occurrences_2(new_list)\n",
    "dashes_list = count_word_occurrences_2(new_list)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0365a6cf-611c-4aa4-9bd0-10223acae0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'for', 'nor', 'but', 'or', 'yet', 'so', 'after', 'although', 'as', 'if', 'long', 'much', 'soon', 'though', 'because', 'before', 'by', 'even', 'if', 'in', 'case', 'lest', 'on', 'once', 'only', 'since', 'that', 'than', 'though', 'till', 'unless', 'until', 'when', 'whenever', 'where', 'wherever', 'while', 'until', 'both', 'and', 'either', 'or', 'neither', 'nor', 'only', 'also', 'whether']\n"
     ]
    }
   ],
   "source": [
    "prepositions_list = open(\"data-science_master/prepositions.json\")\n",
    "# Open and read the JSON file\n",
    "with open(\"data-science_master/prepositions.json\", \"r\") as file:\n",
    "    preposition_list = json.load(file)\n",
    "\n",
    "# Adverbs\n",
    "adverbs = open(\"data-science_master/List of Adverbs.txt\")\n",
    "adverbs_string=str(adverbs.read())\n",
    "adverbs_list = adverbs_string.split(\"\\n\")\n",
    "\n",
    "# Adjectives\n",
    "adjectives = open(\"data-science_master/List of Adjectives.txt\")\n",
    "adjectives_string=str(adjectives.read())\n",
    "adjectives_list = adjectives_string.split(\"\\n\")\n",
    "\n",
    "# Verbs\n",
    "verbs = open(\"data-science_master/List of Verbs.txt\")\n",
    "verbs_string=str(verbs.read())\n",
    "verbs_list = verbs_string.split(\"\\n\")\n",
    "\n",
    "\n",
    "conjunctions = open(\"data-science_master/List of Conjunctions.txt\")\n",
    "conjunctions_string=str(conjunctions.read())\n",
    "conjunctions_list = conjunctions_string.split(\"\\n\")\n",
    "print(conjunctions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bec30b40-e464-495e-bfe5-c124bae29a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources\n",
    "# Manual classification is based on Cambridge Dictionary. Broad overview \n",
    "    # https://dictionary.cambridge.org/grammar/british-grammar/word-classes-and-phrase-classes\n",
    "    \n",
    "# Pronouns are based on 'the free dictionary'\n",
    "    # https://www.thefreedictionary.com/List-of-pronouns.htm\n",
    "\n",
    "# Conjunctions are taken from: \n",
    "    # https://bahanajar.wordpress.com/wp-content/uploads/2017/12/conjunctions2.pdf\n",
    "    \n",
    "# -----------------------------\n",
    "\n",
    "# Some lists were merged for our classification:\n",
    "\n",
    "    # Prepositions_list\n",
    "        # https://gist.github.com/anubsinha/e65538585a5630a936a426667a807269\n",
    "    \n",
    "    # Adverbs & Adjectives & Verbs\n",
    "        # https://github.com/janester/mad_libs/tree/master\n",
    "        \n",
    "    # Conjunctions\n",
    "        # https://gist.github.com/paceaux/f0f62e5a4f1b470d9cd13031607c44e3/revisions\n",
    "        # note that this list was manually adjusted\n",
    "        \n",
    "# all lists can be found in the sources folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e06438bb-94bc-4a6e-b52a-358d7656e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Example text\n",
    "text_example = \"This is an example text. This occurs three times and also at the end like this. Also, it's not what it seems, because it is. I am happy. My name is Jannik. run and running.!\"\n",
    "\n",
    "# Empty Dictionary\n",
    "word_analysis = {}\n",
    "\n",
    "def classify_word(word):\n",
    "    if word.endswith('ly') or word in ['also', 'not', 'how', 'here', 'rather', 'just', 'therefore'] or word in adverbs_list:\n",
    "        return 'Adverb'\n",
    "    elif word.endswith('ing') or word in ['have', 'has', 'be', 'am', 'are', 'is', 'was', 'were', 'do', 'does', 'can', 'could', 'argue', 'will', 'would', 'needed', 'based', 'impacts', 'classifies', 'intoinitiated', 'achieved', 'means', 'forms','subsumed', 'centers', 'fault', 'criticize', 'exceedance', 'organises', 'holds', 'increase', 'decrease', 'entail', 'argued', 'aim ','remains'] or word in verbs_list:\n",
    "        return 'Verb' # problem: how to handle verbs in infinitive? \n",
    "    elif word in ['a', 'an', 'the']: \n",
    "        return 'Article'\n",
    "    elif word in ['and', 'but', 'or', 'yet', 'so', 'after', 'before', 'although', 'even', 'though', 'because', 'as', 'if', 'till', 'until', 'unless', 'once', 'soon', 'while', 'whereas' 'spite', 'despite'] \\\n",
    "    or word in conjunctions_list: \n",
    "        return 'Conjunction'\n",
    "    elif word in preposition_list:\n",
    "        return 'Preposition'\n",
    "    elif word in ['i', 'we', 'you', 'he', 'she', 'it', 'they', # Personal Pronouns / Subject Pronouns\n",
    "            'me', 'us', 'you', 'her', 'him', 'it', 'them', # Object Pronouns\n",
    "            'mine', 'his', 'hers', 'yours', 'ours', 'theirs', # Possessive Pronouns\n",
    "            'my', 'your', 'his', 'her', 'our', 'their', # Possessive Adjectives / Pronominal Adjectives\n",
    "            'this', 'that', 'these', 'those', # Demonstrative Pronouns\n",
    "            'as', 'that', 'what', 'whatever', 'which', 'whichever', 'who', 'whoever', 'whom', 'whomever', 'whose', # Interrogative & Relative Pronouns\n",
    "            'all', 'another', 'any', 'anybody', 'anyone', 'anything', 'both', 'each', 'either', 'everybody', 'everyone', 'everything', 'few', 'many', 'most', 'neither', 'nobody', 'none', 'no one', 'nothing', 'one', 'other', 'others', 'several', 'some', 'somebody', 'someone', 'something', 'such', # Indefinite Pronouns\n",
    "            # Archaic Pronouns in the next line --> just for the sake of completeness\n",
    "            'thou', 'thee', 'thy', 'thine', 'ye'] \\\n",
    "    or word.endswith('self') or word.endswith('selves'): # Reflexive & Intensive Pronouns \n",
    "        return 'Pronoun' \n",
    "    elif word in adjectives_list or word in ['multivariate', 'statistical', 'scientific', 'societal', 'political', 'sustainable', 'major', 'absolute', 'environmental', 'main', 'central', 'important', 'prestigious', 'stationary', 'viablemuch']:\n",
    "        return 'Adjective'\n",
    "    elif word in contractions_list:\n",
    "        return 'Contraction'\n",
    "    elif word in dashes_list:\n",
    "        return 'Dashes'\n",
    "    else:\n",
    "        return 'Noun'  # Assume it's a noun if no other rule matches\n",
    "    \n",
    "# Analyzing & counting the words\n",
    "\n",
    "for word in new_list:\n",
    "    if word: # to avoid empty cells\n",
    "        word_type = classify_word(word)  # Classify the word type\n",
    "        if word in word_analysis:\n",
    "            word_analysis[word]['frequency'] += 1  # Increment frequency\n",
    "        else:\n",
    "            word_analysis[word] = {'type': word_type, 'frequency': 1}  # Initialize the entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4d86879-f5d9-4a80-a508-3791ef7e0d24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "georgescu-roegen\n",
      "domi-nated\n",
      "peer-reviewed\n",
      "renew-ables\n"
     ]
    }
   ],
   "source": [
    "def filter_words_by_type(word_analysis, word_type):\n",
    "    for word, details in word_analysis.items():\n",
    "        if details.get('type') == word_type:\n",
    "            print(f'{word}')\n",
    "filter_words_by_type(word_analysis, 'Dashes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52023ff5-72a0-4404-b13d-119b21f9af4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      type  frequency\n",
      "the                Article         26\n",
      "advantage             Noun          1\n",
      "of             Preposition         21\n",
      "using                 Verb          1\n",
      "our                Pronoun          1\n",
      "...                    ...        ...\n",
      "within         Preposition          1\n",
      "viable                Noun          1\n",
      "schneder              Noun          1\n",
      "understanding         Verb          1\n",
      "degro                 Noun          1\n",
      "\n",
      "[203 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initiating the Dataframe\n",
    "df = pd.DataFrame.from_dict(word_analysis, orient='index')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a877b71c-454c-4d67-ba8b-3757de88ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letters in the Word: [3, 9, 2, 5, 3, 12, 11, 10, 9, 4, 5, 2, 4, 10, 6, 13, 2, 14, 5, 8, 4, 16, 3, 12, 6, 2, 13, 4, 7, 10, 8, 9, 9, 6, 3, 1, 11, 7, 3, 2, 8, 10, 3, 5, 7, 3, 8, 8, 4, 8, 5, 7, 4, 5, 6, 7, 5, 4, 7, 5, 5, 6, 7, 9, 12, 4, 4, 8, 10, 9, 5, 8, 4, 2, 5, 8, 9, 10, 4, 8, 4, 8, 11, 11, 2, 8, 10, 13, 7, 7, 11, 3, 10, 6, 2, 2, 4, 5, 11, 4, 10, 8, 7, 2, 4, 3, 4, 4, 7, 7, 5, 4, 8, 6, 6, 7, 6, 3, 10, 10, 6, 7, 9, 10, 9, 10, 6, 10, 7, 6, 5, 5, 7, 7, 8, 5, 10, 5, 2, 5, 8, 4, 9, 4, 10, 5, 3, 2, 5, 13, 7, 8, 4, 9, 11, 6, 4, 6, 9, 17, 3, 10, 8, 8, 8, 9, 6, 8, 4, 6, 6, 4, 7, 11, 5, 4, 8, 6, 9, 5, 6, 7, 10, 2, 10, 6, 4, 15, 5, 4, 6, 3, 8, 12, 4, 14, 6, 7, 6, 6, 8, 13, 5]\n"
     ]
    }
   ],
   "source": [
    "# word lengths as a function\n",
    "def word_lengths(word_analysis):\n",
    "\n",
    "    # Create a list of word lengths\n",
    "    word_lengths = [len(word) for word in word_analysis]\n",
    "\n",
    "\n",
    "    #change list to dictionary\n",
    "    new_dict_lengths = word_lengths\n",
    "\n",
    "    #return the thing\n",
    "    return new_dict_lengths\n",
    "\n",
    "    #print the dictionary\n",
    "\n",
    "    #call the funtion\n",
    "lengths_of_word = word_lengths(word_analysis)\n",
    "print(\"Letters in the Word:\", lengths_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3aa0017-827f-43cd-ba81-9e5e3adf38dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      type  frequency  length\n",
      "the                Article         26       3\n",
      "advantage             Noun          1       9\n",
      "of             Preposition         21       2\n",
      "using                 Verb          1       5\n",
      "our                Pronoun          1       3\n",
      "...                    ...        ...     ...\n",
      "within         Preposition          1       6\n",
      "viable                Noun          1       6\n",
      "schneder              Noun          1       8\n",
      "understanding         Verb          1      13\n",
      "degro                 Noun          1       5\n",
      "\n",
      "[203 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df['length'] = lengths_of_word\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
